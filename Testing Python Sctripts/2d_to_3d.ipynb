{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clip coords = tensor([ 39.8372, -12.9904,  21.3043,  21.5000], device='cuda:0')\n",
      "ndc_coords = tensor([ 1.8529, -0.6042,  0.9909], device='cuda:0')\n",
      "cam_space_val = tensor([ 23.0000,  -7.5000, -21.5000,   1.0000], device='cuda:0')\n",
      "Projected 2D Pixel Coordinates: (1141.1565780639648, 481.26112818717957), Depth: -21.500001907348633\n",
      "Clip Coordinates for Reconstruction: tensor([  1.8529,  -0.6042, -21.5000,   1.0000], device='cuda:0')\n",
      "Reconstructed 3D World Coordinates: tensor([ 1.0006e+00,  8.2546e-04, -4.9967e+00], device='cuda:0')\n",
      "Original 3D World Coordinates: tensor([2.5000, 2.0000, 3.0000], device='cuda:0')\n",
      "Comparison: tensor([2.5000, 2.0000, 3.0000], device='cuda:0') vs tensor([ 1.0006e+00,  8.2546e-04, -4.9967e+00], device='cuda:0')\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Reconstruction failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 91\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Check if original and reconstructed points match\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComparison: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworld_point[:\u001b[38;5;241m3\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m vs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworld_point_reconstructed[:\u001b[38;5;241m3\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(world_point[:\u001b[38;5;241m3\u001b[39m], world_point_reconstructed[:\u001b[38;5;241m3\u001b[39m], atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReconstruction failed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3D Reconstruction from 2D coordinates is successful!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Reconstruction failed"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Function to create a view matrix given rotation (R) and translation (T)\n",
    "def get_view_matrix(R, T):\n",
    "    view_matrix = torch.eye(4).cuda()\n",
    "    view_matrix[:3, :3] = R  # Rotation part\n",
    "    view_matrix[:3, 3] = T   # Translation part\n",
    "    return torch.inverse(view_matrix)  # Inverting to get world-to-view\n",
    "\n",
    "# Function to create a projection matrix given near, far, fovX, and aspect ratio\n",
    "def get_projection_matrix(znear, zfar, fovX, fovY):\n",
    "    # Convert degrees to radians manually\n",
    "    radians_per_degree = torch.tensor(3.14159265358979323846 / 180.0).cuda()  # π/180\n",
    "    tan_half_fovX = torch.tan(fovX * radians_per_degree / 2)\n",
    "    tan_half_fovY = torch.tan(fovY * radians_per_degree / 2)\n",
    "\n",
    "    projection_matrix = torch.zeros((4, 4)).cuda()\n",
    "    projection_matrix[0, 0] = 1 / tan_half_fovX  # X scaling\n",
    "    projection_matrix[1, 1] = 1 / tan_half_fovY  # Y scaling\n",
    "    projection_matrix[2, 2] = -(zfar + znear) / (zfar - znear)  # Z scaling for perspective\n",
    "    projection_matrix[2, 3] = -(2 * zfar * znear) / (zfar - znear)\n",
    "    projection_matrix[3, 2] = -1  # Perspective division\n",
    "    return projection_matrix\n",
    "\n",
    "# Define parameters\n",
    "znear = 0.1\n",
    "zfar = 1000.0\n",
    "fovX, fovY = 60.0, 60.0\n",
    "width, height = 800, 600\n",
    "\n",
    "# Example rotation and translation for view matrix\n",
    "R = torch.tensor([[1,0,1],[2,3,1],[1,2,0]]).cuda()\n",
    "T = torch.tensor([1, 0, -5]).cuda()\n",
    "\n",
    "# Generate view and projection matrices\n",
    "view_matrix = get_view_matrix(R, T).cuda()\n",
    "projection_matrix = get_projection_matrix(znear, zfar, fovX, fovY).cuda()\n",
    "\n",
    "# Combine to get full projection transform\n",
    "full_proj_transform = projection_matrix @ view_matrix\n",
    "\n",
    "# Step 2: Define a 3D point in world space and project to 2D\n",
    "world_point = torch.tensor([2.5, 2.0, 3.0, 1.0]).cuda()  # Homogeneous coordinates\n",
    "clip_coords = full_proj_transform @ world_point  # Transform to clip space\n",
    "\n",
    "# Normalize to NDC\n",
    "ndc_coords = clip_coords[:3] / clip_coords[3]\n",
    "\n",
    "print(f\"Clip coords = {clip_coords}\")\n",
    "print(f\"ndc_coords = {ndc_coords}\")\n",
    "\n",
    "# Map NDC to pixel coordinates\n",
    "pixel_x = (ndc_coords[0].item() * 0.5 + 0.5) * width\n",
    "pixel_y = (1.0 - (ndc_coords[1].item() * 0.5 + 0.5)) * height\n",
    "\n",
    "cam_space_val =  view_matrix @ world_point\n",
    "print(f\"cam_space_val = {cam_space_val}\")\n",
    "depth_value = cam_space_val[2] #clip_coords[2].item()  # Using depth from the clip coordinates\n",
    "\n",
    "\n",
    "print(f\"Projected 2D Pixel Coordinates: ({pixel_x}, {pixel_y}), Depth: {depth_value}\")\n",
    "\n",
    "# Step 3: Reverse the process to retrieve 3D coordinates from 2D pixel\n",
    "# Convert pixel back to NDC\n",
    "ndc_x = (2 * pixel_x / width) - 1\n",
    "ndc_y = 1 - (2 * pixel_y / height)\n",
    "\n",
    "# Use depth value directly\n",
    "depth_in_camera_space = depth_value  # This should correspond to (view_matrix * world_point)[2]\n",
    "\n",
    "# Create clip space coordinates with the known depth\n",
    "clip_coords_reconstructed = torch.tensor([ndc_x, ndc_y, depth_in_camera_space, 1.0]).cuda()\n",
    "\n",
    "# Back-project to camera space\n",
    "full_proj_inv = torch.inverse(full_proj_transform)\n",
    "\n",
    "# Check intermediate values for debugging\n",
    "print(f\"Clip Coordinates for Reconstruction: {clip_coords_reconstructed}\")\n",
    "\n",
    "world_point_reconstructed = full_proj_inv @ clip_coords_reconstructed\n",
    "\n",
    "# Normalize homogeneous coordinates without in-place division\n",
    "normalization_factor = world_point_reconstructed[3]\n",
    "world_point_reconstructed = world_point_reconstructed / normalization_factor  # Divide by the normalization factor\n",
    "\n",
    "print(f\"Reconstructed 3D World Coordinates: {world_point_reconstructed[:3]}\")\n",
    "print(f\"Original 3D World Coordinates: {world_point[:3]}\")\n",
    "\n",
    "# Check if original and reconstructed points match\n",
    "print(f\"Comparison: {world_point[:3]} vs {world_point_reconstructed[:3]}\")\n",
    "assert torch.allclose(world_point[:3], world_point_reconstructed[:3], atol=1e-5), \"Reconstruction failed\"\n",
    "print(\"3D Reconstruction from 2D coordinates is successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip_coords =tensor([ 5.9631, 11.0088, -4.2008, -4.0000])\n",
      "cam_space_val = tensor([ 5., 11.,  4.,  1.])\n",
      "Projected 2D Pixel Coordinates: (-196.3052749633789, 1125.657033920288), Depth: 4.0\n",
      "Clip Coordinates for Reconstruction: tensor([ 5.9631, 11.0088, -4.0000, -4.0000])\n",
      "view_matrix = tensor([[-1., -0.,  2., 12.],\n",
      "        [-2.,  1.,  2., 13.],\n",
      "        [ 1.,  0., -1., -7.],\n",
      "        [ 0.,  0.,  0.,  1.]])\n",
      "projection_matrix = tensor([[ 1.1926,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  1.0008,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000, -1.0002, -0.2000],\n",
      "        [ 0.0000,  0.0000, -1.0000,  0.0000]])\n",
      "full_proj_transform = tensor([[-1.1926,  0.0000,  2.3852, 14.3113],\n",
      "        [-2.0016,  1.0008,  2.0016, 13.0104],\n",
      "        [-1.0002,  0.0000,  1.0002,  6.8014],\n",
      "        [-1.0000,  0.0000,  1.0000,  7.0000]])\n",
      "full_proj_inv = tensor([[ 8.3850e-01,  2.3842e-07, -9.9990e+00,  8.0010e+00],\n",
      "        [ 2.3842e-07,  9.9920e-01, -4.9995e+00,  3.0005e+00],\n",
      "        [ 8.3849e-01,  9.5367e-07,  2.4997e+01, -2.6002e+01],\n",
      "        [ 4.7684e-07,  0.0000e+00, -4.9995e+00,  5.0005e+00]])\n",
      "tensor([[-1.1926,  0.0000,  2.3852],\n",
      "        [-2.0016,  1.0008,  2.0016],\n",
      "        [-1.0000,  0.0000,  1.0000]])\n",
      "tensor([ -8.3483,  -2.0016, -11.0000])\n",
      "Reconstructed 3D World Coordinates: tensor([15., 20.,  4.])\n",
      "Original 3D World Coordinates: tensor([15., 20.,  4.])\n",
      "Comparison: tensor([0., 0., 0.])\n",
      "3D Reconstruction from 2D coordinates is successful!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2268036/2045222530.py:87: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  x, res, r, s = np.linalg.lstsq(p, b)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Function to create a view matrix given rotation (R) and translation (T)\n",
    "def get_view_matrix(R, T):\n",
    "    view_matrix = torch.eye(4)\n",
    "    view_matrix[:3, :3] = R  # Rotation part\n",
    "    view_matrix[:3, 3] = T   # Translation part\n",
    "    return torch.inverse(view_matrix)  # Inverting to get world-to-view\n",
    "\n",
    "# Function to create a projection matrix given near, far, fovX, and aspect ratio\n",
    "def get_projection_matrix(znear, zfar, fovX, fovY):\n",
    "    # Convert degrees to radians manually\n",
    "    radians_per_degree = torch.tensor(3.14 / 180.0) # π/180\n",
    "    tan_half_fovX = torch.tan(fovX * radians_per_degree / 2)\n",
    "    tan_half_fovY = torch.tan(fovY * radians_per_degree / 2)\n",
    "\n",
    "    projection_matrix = torch.zeros((4, 4))\n",
    "    projection_matrix[0, 0] = 1 / tan_half_fovX  # X scaling\n",
    "    projection_matrix[1, 1] = 1 / tan_half_fovY  # Y scaling\n",
    "    projection_matrix[2, 2] = -(zfar + znear) / (zfar - znear)  # Z scaling for perspective\n",
    "    projection_matrix[2, 3] = -(2 * zfar * znear) / (zfar - znear)\n",
    "    projection_matrix[3, 2] = -1  # Perspective division\n",
    "    return projection_matrix\n",
    "\n",
    "\n",
    "znear = 0.1\n",
    "zfar = 1000.0\n",
    "fovX, fovY = 80.0, 90.0\n",
    "width, height = 800, 600\n",
    "\n",
    "\n",
    "R = torch.tensor([[1,0,2],[0,1,2],[1,0,1]])\n",
    "T = torch.tensor([2, 1, -5])\n",
    "\n",
    "\n",
    "view_matrix = get_view_matrix(R, T)\n",
    "projection_matrix = get_projection_matrix(znear, zfar, fovX, fovY)\n",
    "\n",
    "\n",
    "full_proj_transform = projection_matrix @ view_matrix\n",
    "\n",
    "\n",
    "world_point = torch.tensor([15.0, 20.0, 4.0, 1.0]) \n",
    "clip_coords = full_proj_transform @ world_point \n",
    "print(f\"clip_coords ={clip_coords}\")\n",
    "\n",
    "# Normalize to NDC\n",
    "ndc_coords = clip_coords[:3] / clip_coords[3]\n",
    "\n",
    "# Map NDC to pixel coordinates\n",
    "pixel_x = (ndc_coords[0].item() * 0.5 + 0.5) * width\n",
    "pixel_y = (1.0 - (ndc_coords[1].item() * 0.5 + 0.5)) * height\n",
    "\n",
    "cam_space_val = view_matrix @ world_point\n",
    "print(f\"cam_space_val = {cam_space_val}\")\n",
    "depth_value = cam_space_val[2].item() \n",
    "\n",
    "print(f\"Projected 2D Pixel Coordinates: ({pixel_x}, {pixel_y}), Depth: {depth_value}\")\n",
    "\n",
    "ndc_x = (2 * pixel_x / width) - 1\n",
    "ndc_y = 1 - (2 * pixel_y / height)\n",
    "\n",
    "\n",
    "depth_in_camera_space = depth_value  # Using positive depth for reconstruction\n",
    "\n",
    "\n",
    "clip_coords_reconstructed = -1*torch.tensor([ndc_x, ndc_y, 1.0, 1.0])*depth_in_camera_space\n",
    "\n",
    "# Back-project to camera space\n",
    "full_proj_inv = torch.inverse(full_proj_transform)\n",
    "\n",
    "\n",
    "print(f\"Clip Coordinates for Reconstruction: {clip_coords_reconstructed}\")\n",
    "\n",
    "\n",
    "print(f\"view_matrix = {view_matrix}\")\n",
    "print(f\"projection_matrix = {projection_matrix}\")\n",
    "print(f\"full_proj_transform = {full_proj_transform}\")\n",
    "print(f\"full_proj_inv = {full_proj_inv}\")\n",
    "\n",
    "p = full_proj_transform[torch.tensor([0,1,3])][:,:3]\n",
    "print(p)\n",
    "\n",
    "b = clip_coords_reconstructed[:3]-full_proj_transform[torch.tensor([0,1,3])][:,3]\n",
    "print(b)\n",
    "x, res, r, s = np.linalg.lstsq(p, b)\n",
    "\n",
    "world_point_reconstructed_rounded = torch.round(torch.tensor(x)*1000)/1000\n",
    "\n",
    "print(f\"Reconstructed 3D World Coordinates: {world_point_reconstructed_rounded}\")\n",
    "print(f\"Original 3D World Coordinates: {world_point[:3]}\")\n",
    "\n",
    "# Check if original and reconstructed points match with a tolerance\n",
    "tolerance = 1e-2  # Allow a small error margin\n",
    "comparison = torch.abs(world_point[:3] - world_point_reconstructed_rounded)\n",
    "print(f\"Comparison: {comparison}\")\n",
    "\n",
    "# Check if the difference is within the allowed tolerance\n",
    "assert torch.all(comparison < tolerance), \"Reconstruction failed\"\n",
    "print(\"3D Reconstruction from 2D coordinates is successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new means_2d = \n",
      " tensor([[-0.9991, -0.9995],\n",
      "        [-0.9991, -0.9995],\n",
      "        [-0.9991, -0.9995],\n",
      "        [ 0.0560,  0.0835],\n",
      "        [-0.9991, -0.9995],\n",
      "        [-0.9991, -0.9995],\n",
      "        [-0.1035,  0.0623],\n",
      "        [-0.9991, -0.9995],\n",
      "        [-0.6618,  0.0999],\n",
      "        [-0.9991, -0.9995]], grad_fn=<CopySlices>)\n",
      "p \n",
      " tensor([[ 1.2347,  1.0517, -0.7158],\n",
      "        [-0.3155,  0.7530,  0.5621],\n",
      "        [ 0.6431, -0.2664,  0.7179]])\n",
      "b \n",
      " tensor([[-0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000],\n",
      "        [ 0.3029,  0.4519,  5.4124],\n",
      "        [-0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000],\n",
      "        [-0.3798,  0.2288,  3.6705],\n",
      "        [-0.0000, -0.0000,  0.0000],\n",
      "        [-6.2823,  0.9481,  9.4935],\n",
      "        [-0.0000, -0.0000,  0.0000]])\n",
      "b \n",
      " tensor([[-0.8998,  1.8877, -3.9551],\n",
      "        [-0.8998,  1.8877, -3.9551],\n",
      "        [-0.8998,  1.8877, -3.9551],\n",
      "        [-0.5969,  2.3396,  1.4573],\n",
      "        [-0.8998,  1.8877, -3.9551],\n",
      "        [-0.8998,  1.8877, -3.9551],\n",
      "        [-1.2796,  2.1165, -0.2846],\n",
      "        [-0.8998,  1.8877, -3.9551],\n",
      "        [-7.1821,  2.8358,  5.5384],\n",
      "        [-0.8998,  1.8877, -3.9551]])\n",
      "Reconstructed 3d \n",
      " [[-3.50350118  2.19946432 -1.55461907]\n",
      " [-3.50350118  2.19946432 -1.55461907]\n",
      " [-3.50350118  2.19946432 -1.55461907]\n",
      " [-0.04860705  1.20507169  2.52067137]\n",
      " [-3.50350118  2.19946432 -1.55461907]\n",
      " [-3.50350118  2.19946432 -1.55461907]\n",
      " [-1.3654896   1.26975775  1.29796612]\n",
      " [-3.50350118  2.19946432 -1.55461907]\n",
      " [-0.17017402 -1.70556819  7.23426008]\n",
      " [-3.50350118  2.19946432 -1.55461907]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2268036/2756429043.py:53: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  X[i], res, r, s = np.linalg.lstsq(p, b[i])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "\n",
    "def pix2ndc(p,s):\n",
    "    return (2*p +1)/s -1\n",
    "\n",
    "h, w = 1921, 1073\n",
    "\n",
    "projection_full = torch.tensor([[ 1.2347, -0.3155,  0.6432,  0.6431],\n",
    "        [ 1.0517,  0.7530, -0.2665, -0.2664],\n",
    "        [-0.7158,  0.5621,  0.7180,  0.7179],\n",
    "        [ 0.8998, -1.8877,  3.9454,  3.9551]])\n",
    "means_2d = torch.tensor([[   0.0000,    0.0000],\n",
    "        [   0.0000,    0.0000],\n",
    "        [   0.0000,    0.0000],\n",
    "        [ 566.0210, 1040.1997],\n",
    "        [   0.0000,    0.0000],\n",
    "        [   0.0000,    0.0000],\n",
    "        [ 480.4791, 1019.8796],\n",
    "        [   0.0000,    0.0000],\n",
    "        [ 180.9700, 1055.9210],\n",
    "        [   0.0000,    0.0000]], requires_grad=True)\n",
    "\n",
    "means_2d_new = torch.ones_like(means_2d)\n",
    "means_2d_new[:,0], means_2d_new[:,1] = pix2ndc(means_2d[:,0],w), pix2ndc(means_2d[:,1],h)\n",
    "\n",
    "print(f\"new means_2d = \\n {means_2d_new}\")\n",
    "\n",
    "depths = torch.tensor([0.0000, 0.0000, 0.0000, 5.4124, 0.0000, 0.0000, 3.6705, 0.0000, 9.4935,\n",
    "        0.0000])\n",
    "means_3d = torch.tensor([[-1.9686, -5.0952,  8.4925],\n",
    "        [-7.0956, -0.8330,  3.4324],\n",
    "        [-3.6041,  3.3491,  2.8043],\n",
    "        [-0.0486,  1.2050,  2.5206],\n",
    "        [-7.4773, -0.5681,  3.6143],\n",
    "        [-7.0176, -0.3949,  3.8820],\n",
    "        [-1.3654,  1.2697,  1.2979],\n",
    "        [-6.6321, -2.4209,  0.9914],\n",
    "        [-0.1703, -1.7058,  7.2341],\n",
    "        [-1.7995, -5.9542,  7.3705]])\n",
    "\n",
    "def get_mean_3d(mean_2d, depth, projection_full):\n",
    "    size = means_2d.shape[0]\n",
    "    p = projection_full.T[torch.tensor([0,1,3])][:,:3]\n",
    "    print(f\"p \\n {p}\")\n",
    "    b = torch.ones(size,3)\n",
    "    b[:,:2], b[:,2] = mean_2d.detach()*depth.reshape(size,1), depth\n",
    "    print(f\"b \\n {b}\")\n",
    "    b = b - projection_full.T[torch.tensor([0,1,3])][:,3]\n",
    "    print(f\"b \\n {b}\")\n",
    "    X = np.ones((size,3))\n",
    "    for i in range(size):\n",
    "        X[i], res, r, s = np.linalg.lstsq(p, b[i])\n",
    "    print(f\"Reconstructed 3d \\n {X}\")\n",
    "    return torch.tensor(X, requires_grad=True) \n",
    "\n",
    "x = get_mean_3d(means_2d_new, depths, projection_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.5035,  2.1995, -1.5546],\n",
      "        [-3.5035,  2.1995, -1.5546],\n",
      "        [-3.5035,  2.1995, -1.5546],\n",
      "        [-0.0486,  1.2051,  2.5207],\n",
      "        [-3.5035,  2.1995, -1.5546],\n",
      "        [-3.5035,  2.1995, -1.5546],\n",
      "        [-1.3655,  1.2698,  1.2980],\n",
      "        [-3.5035,  2.1995, -1.5546],\n",
      "        [-0.1702, -1.7056,  7.2343],\n",
      "        [-3.5035,  2.1995, -1.5546]], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_mean_3d(mean_2d, depth, projection_full, w, h):\n",
    "    \"\"\"\n",
    "    Inputs:-\n",
    "        mean_2d - 2d prixel centroid values of blobs\n",
    "        depth - Average depth tensor (use knn to get the neighbours and get an average depth value for blobs)\n",
    "        projection_full - Projection_full matrix of camera viewpoint (projection_mat @ view_mat)\n",
    "        w - image width\n",
    "        h - image hight\n",
    "    Output\n",
    "        X - 3d coordinates w.r.t world coordinates\n",
    "    \"\"\"\n",
    "    size = mean_2d.shape[0]\n",
    "    # Converting pixel values to ndc\n",
    "    means_2d_new = torch.ones_like(mean_2d)\n",
    "    means_2d_new[:,0], means_2d_new[:,1] = pix2ndc(mean_2d[:,0],w), pix2ndc(mean_2d[:,1],h)\n",
    "\n",
    "    p = projection_full.T[torch.tensor([0,1,3])][:,:3]  # In 3d gaussian splatting, they have used column prioritizing\n",
    "    b = torch.ones(size,3)\n",
    "    b[:,:2] = means_2d_new.detach()*depth.reshape(size,1)\n",
    "    b[:,2] =  depth\n",
    "    b = b - projection_full.T[torch.tensor([0,1,3])][:,3]\n",
    "    X = np.ones((size,3))\n",
    "    for i in range(size):\n",
    "        X[i], res, r, s = torch.linalg.lstsq(p, b[i], rcond=None)\n",
    "    return torch.tensor(X, requires_grad=True) \n",
    "\n",
    "x = get_mean_3d(means_2d, depths, projection_full,w,h)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_3d = torch.tensor([[-1.9686, -5.0952,  8.4925],\n",
    "        [-7.0956, -0.8330,  3.4324],\n",
    "        [-3.6041,  3.3491,  2.8043],\n",
    "        [-0.0486,  1.2050,  2.5206],\n",
    "        [-7.4773, -0.5681,  3.6143],\n",
    "        [-7.0176, -0.3949,  3.8820],\n",
    "        [-1.3654,  1.2697,  1.2979],\n",
    "        [-6.6321, -2.4209,  0.9914],\n",
    "        [-0.1703, -1.7058,  7.2341],\n",
    "        [-1.7995, -5.9542,  7.3705]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time 1: 0.001003265380859375 seconds\n",
      "tensor([[-3.5035,  2.1995, -1.5546],\n",
      "        [-3.5035,  2.1995, -1.5546],\n",
      "        [-3.5035,  2.1995, -1.5546],\n",
      "        [-0.0486,  1.2051,  2.5207],\n",
      "        [-3.5035,  2.1995, -1.5546],\n",
      "        [-3.5035,  2.1995, -1.5546],\n",
      "        [-1.3655,  1.2698,  1.2980],\n",
      "        [-3.5035,  2.1995, -1.5546],\n",
      "        [-0.1702, -1.7056,  7.2343],\n",
      "        [-3.5035,  2.1995, -1.5546]], dtype=torch.float64, requires_grad=True)\n",
      "Execution time 2: 0.0007510185241699219 seconds\n",
      "tensor([[-3.5035,  2.1995, -1.5546],\n",
      "        [-3.5035,  2.1995, -1.5546],\n",
      "        [-3.5035,  2.1995, -1.5546],\n",
      "        [-0.0486,  1.2051,  2.5207],\n",
      "        [-3.5035,  2.1995, -1.5546],\n",
      "        [-3.5035,  2.1995, -1.5546],\n",
      "        [-1.3655,  1.2698,  1.2980],\n",
      "        [-3.5035,  2.1995, -1.5546],\n",
      "        [-0.1702, -1.7056,  7.2343],\n",
      "        [-3.5035,  2.1995, -1.5546]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def get_mean_3d_cuda(blobs_xy, depth, projection_full, w, h):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        blobs_xy - 2D pixel centroid values of blobs\n",
    "        depth - Average depth tensor (use knn to get neighbors and get an average depth value for blobs)\n",
    "        projection_full - Projection matrix of camera viewpoint (projection_mat @ view_mat)\n",
    "        w - Image width\n",
    "        h - Image height\n",
    "    Output:\n",
    "        X - 3D coordinates with respect to world coordinates\n",
    "    \"\"\"\n",
    "    device = blobs_xy.device  # Assuming blobs_xy, depth, and projection_full are on the same device (CUDA)\n",
    "\n",
    "    # Number of blobs\n",
    "    size = blobs_xy.shape[0]\n",
    "\n",
    "    # Converting pixel values to NDC (Normalized Device Coordinates)\n",
    "    blobs_xy_new = torch.ones_like(blobs_xy, device=device)\n",
    "    blobs_xy_new[:, 0], blobs_xy_new[:, 1] = pix2ndc(blobs_xy[:, 0], w), pix2ndc(blobs_xy[:, 1], h)\n",
    "\n",
    "    # Preparing matrices for Ax = b\n",
    "    p = projection_full.T[torch.tensor([0, 1, 3], device=device)][:, :3]  # Column prioritizing\n",
    "    b = torch.ones(size, 3, device=device)  # b matrix in Ax = b\n",
    "    b[:, :2] = blobs_xy_new * depth.view(size, 1)\n",
    "    b[:, 2] = depth\n",
    "\n",
    "    # Adjust b by subtracting translation part of projection\n",
    "    b = b - projection_full.T[torch.tensor([0, 1, 3], device=device)][:, 3]\n",
    "\n",
    "    # Solve Ax = b in batch mode\n",
    "    # p needs to be expanded to match the size of b for batch lstsq\n",
    "    p_batch = p.unsqueeze(0).expand(size, -1, -1)  # Expanding p to shape (size, 3, 3)\n",
    "    b_batch = b.unsqueeze(-1)  # Making b shape (size, 3, 1)\n",
    "\n",
    "    # Use torch.linalg.lstsq on CUDA with the `gels` driver (default for CUDA)\n",
    "    X= torch.linalg.lstsq(p_batch, b_batch).solution\n",
    "\n",
    "    # Remove the extra dimension added for lstsq compatibility\n",
    "    X = X.squeeze(-1)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "start_time_1 = time.time()\n",
    "x = get_mean_3d(means_2d, depths, projection_full,w,h)\n",
    "end_time_1 = time.time()\n",
    "\n",
    "execution_time = end_time_1 - start_time_1\n",
    "print(\"Execution time 1:\", execution_time, \"seconds\")\n",
    "print(x)\n",
    "\n",
    "means_2d = means_2d.to(device='cuda')\n",
    "depths = depths.to(device='cuda')\n",
    "projection_full = projection_full.to(device = 'cuda')\n",
    "\n",
    "start_time_2 = time.time()\n",
    "x = get_mean_3d_cuda(means_2d, depths, projection_full,w,h)\n",
    "end_time_2 = time.time()\n",
    "\n",
    "execution_time = end_time_2 - start_time_2\n",
    "print(\"Execution time 2:\", execution_time, \"seconds\")\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ges",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
